{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tutorial mlp - exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gkreplak/IVADO_Mila_Mooc/blob/master/tutorial_mlp_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mipSoOVlavkb"
      },
      "source": [
        "GK\n",
        "\n",
        "# IVADO/MILA DEEP LEARNING SCHOOL\n",
        "# 4th edition (Fall 2019)\n",
        "# Tutorial : Categorical data with multilayer perceptron (MLP)\n",
        "\n",
        "## Authors: \n",
        "\n",
        "Arsène Fansi Tchango <arsene.fansi.tchango@mila.quebec>\n",
        "\n",
        "Gaétan Marceau Caron <gaetan.marceau.caron@mila.quebec>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sLHwvggEZERd"
      },
      "source": [
        "## Preface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JKNGtQkkohiM"
      },
      "source": [
        "This tutorial introduces the practical aspects of Deep Learning through the realization of a simple end-to-end project. We will use the deep learning library <a href=\"https://pytorch.org/\"> `PyTorch`</a>, which is well-known for its ease of use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NOD70vdvvtin"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq9FwFVnQihX",
        "colab_type": "text"
      },
      "source": [
        "## Loading packages and using GPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reCpBfp1Qcrt",
        "colab_type": "text"
      },
      "source": [
        "Before we start, we install the necessary packages for the tutorial by using pip. To do this, execute the following cell by selecting it and using `shift+Enter`. This step may take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c5AlBPjnvzNh",
        "outputId": "3005f7ce-59dd-47a1-97e3-29a1d0f671c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip3 install 'torch==1.1.0' 'torchvision==0.3.0' 'Pillow==4.3.0' 'matplotlib==3.1.1' 'imgaug==0.2.5'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/60/f685fb2cfb3088736bafbc9bdbb455327bdc8906b606da9c9a81bae1c81e/torch-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (676.9MB)\n",
            "\u001b[K     |████████████████████████████████| 676.9MB 26kB/s \n",
            "\u001b[?25hCollecting torchvision==0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/45/0f2f3062c92d9cf1d5d7eabd3cae88cea9affbd2b17fb1c043627838cb0a/torchvision-0.3.0-cp36-cp36m-manylinux1_x86_64.whl (2.6MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6MB 53.2MB/s \n",
            "\u001b[?25hCollecting Pillow==4.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/5c/44a8f05da34cbb495e5330825c2204b9fa761357c87bc0bc1785b1d76e41/Pillow-4.3.0-cp36-cp36m-manylinux1_x86_64.whl (5.8MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8MB 25.4MB/s \n",
            "\u001b[?25hCollecting matplotlib==3.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/4f/dd381ecf6c6ab9bcdaa8ea912e866dedc6e696756156d8ecc087e20817e2/matplotlib-3.1.1-cp36-cp36m-manylinux1_x86_64.whl (13.1MB)\n",
            "\u001b[K     |████████████████████████████████| 13.1MB 249kB/s \n",
            "\u001b[?25hCollecting imgaug==0.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/60/a06a48d85a7e9062f5870347a3e3e953da30b37928d43b380c949bca458a/imgaug-0.2.5.tar.gz (562kB)\n",
            "\u001b[K     |████████████████████████████████| 563kB 51.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.1.0) (1.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.3.0) (1.12.0)\n",
            "Collecting olefile\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/81/e1ac43c6b45b4c5f8d9352396a14144bba52c8fec72a80f425f6a4d653ad/olefile-0.46.zip (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 56.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.1) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.1) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.1) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.1) (2.4.7)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.5) (1.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug==0.2.5) (0.16.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.5) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.5) (2.4)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug==0.2.5) (2.4.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug==0.2.5) (4.4.2)\n",
            "Building wheels for collected packages: imgaug, olefile\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgaug: filename=imgaug-0.2.5-cp36-none-any.whl size=561439 sha256=f76ad2e0d1be253f49415e52b9e96d307af22b9fe0c956e68d0cfd600736a9cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/48/c8/ca3345e8582a078de94243996e148377ef66fdb845557bae0b\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35417 sha256=f4ce2a75eb510dd515637084bb7480ea2c61cf222123bc56b2bf516f09e4bd8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/f4/11/bc4166107c27f07fd7bba707ffcb439619197638a1ac986df3\n",
            "Successfully built imgaug olefile\n",
            "Installing collected packages: torch, olefile, Pillow, torchvision, matplotlib, imgaug\n",
            "  Found existing installation: torch 1.5.0+cu101\n",
            "    Uninstalling torch-1.5.0+cu101:\n",
            "      Successfully uninstalled torch-1.5.0+cu101\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "  Found existing installation: torchvision 0.6.0+cu101\n",
            "    Uninstalling torchvision-0.6.0+cu101:\n",
            "      Successfully uninstalled torchvision-0.6.0+cu101\n",
            "  Found existing installation: matplotlib 3.2.1\n",
            "    Uninstalling matplotlib-3.2.1:\n",
            "      Successfully uninstalled matplotlib-3.2.1\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "Successfully installed Pillow-4.3.0 imgaug-0.2.5 matplotlib-3.1.1 olefile-0.46 torch-1.1.0 torchvision-0.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "matplotlib",
                  "mpl_toolkits"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "djF9gjzLwsDB"
      },
      "source": [
        "Now, import all the modules we will use for this tutorial by running the next cell:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w9LnNnxBw0wC",
        "outputId": "66c2ccd1-4c82-492b-fcc9-10b2d4cb6bdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "seed = 1234\n",
        "np.random.seed(seed) # Set the random seed of numpy for the data split.\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n",
        "\n",
        "\n",
        "print(\"Torch version: \", torch.__version__)\n",
        "print(\"GPU Available: {}\".format(use_gpu))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Torch version:  1.1.0\n",
            "GPU Available: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZKzgFV9Favkt"
      },
      "source": [
        "## PyTorch in a nutshell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vrus_-F0avkt"
      },
      "source": [
        "*PyTorch* is a Python library that supports a vibrant ecosystem of tools and libraries for ML in vision, NLP, and more. It provides two high-level features:\n",
        "<ul>\n",
        "<li> operations on <a href=\"https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py\">tensors</a> (such as NumPy) with GPU support,</li>\n",
        "<li> operations for creating and optimizing computational graphs with an automatic differentiation system called <a href=\"https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py\">Autograd</a>.</li>\n",
        "</ul>\n",
        "\n",
        "<a href=\"https://pytorch.org/docs/stable/torch.html\">PyTorch docs</a> contain the API documentation and <a href=\"https://pytorch.org/tutorials/\">many tutorials</a>.\n",
        "Also, PyTorch offers several data processing utilities. One of these utilities is the class <a href=\"http://pytorch.org/docs/master/data.html#\"> `torch.utils.data.Dataset`</a> which offers an easy to use interface to handle a data set. For more information, please refer to the following urls: \n",
        "<ul>\n",
        "<li>PyTorch data sets: <a href=\"http://pytorch.org/docs/master/data.html\"> PyTorch - datasets</a>.</li>\n",
        "<li>A tutorial for loading data: <a href=\"http://pytorch.org/tutorials/beginner/data_loading_tutorial.html\"> PyTorch - data loading tutorial</a>.</li>\n",
        "</ul>\n",
        "\n",
        "<a href=\"http://pytorch.org/docs/master/cuda.html#module-torch.cuda\">`torch.cuda`</a> is a package that provides the same functions as CPU tensors but for  CUDA tensors, which are used for GPU computing. <a href=\"http://pytorch.org/docs/master/cuda.html#torch.cuda.is_available\">`torch.cuda.is_available()`</a> returns a boolean indicating if CUDA is currently available. Finally, we recommend using a `device` variable that identifies the device on which you want to perform computations. We can assign a tensor to a device with the method `.to(device)`. By default, the tensors are CPU tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qm122vNmq92L"
      },
      "source": [
        "## Ingredients for a proof of concept (POC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nqvhR0ebavmE"
      },
      "source": [
        "To realize a ML POC, you need:\n",
        "<ul>\n",
        "<li>a task description as well as data to support it,</li>\n",
        "<li>an evaluation metric to assess the performance of a model,</li>\n",
        "<li>a model description,</li>\n",
        "<li>a cost function to be optimized,</li>\n",
        "<li>an optimizer that adjusts the parameters of the model.</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y8_pfpu2f6AO"
      },
      "source": [
        "# How to prepare the dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "B5piZxYUhSzq"
      },
      "source": [
        "Our task is to predict whether or not a passenger survived the sinking of the Titanic based on passenger data only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y4GuYNDFavlU"
      },
      "source": [
        "## Titanic dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NiOJx2ytavlU"
      },
      "source": [
        "We can download the Titanic dataset at the following address: https://github.com/afansi/winterschool18/blob/master/titanic3.csv?raw=true.<br/>\n",
        "This dataset provides information on the fate of 1309 passengers of the first and only journey of the liner \"RMS Titanic\", summarized by economic status (class), gender, age, family information, and survival. The Kaggle platform also uses this dataset as an introduction to classical machine learning. Here, we use it to introduce more advanced concepts related to PyTorch and Deep Learning.\n",
        "\n",
        "We use the library <a href=\"https://pandas.pydata.org/\">Pandas</a> to load the dataset into memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bX_RSiffavlW",
        "outputId": "8740376c-0e64-499a-aed2-d2fe2c0e7e41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "titanic_df = pd.read_csv(\n",
        "    'https://github.com/afansi/winterschool18/blob/master/titanic3.csv?raw=true', \n",
        "    sep='\\t', \n",
        "    index_col=None, \n",
        "    na_values=['NA']\n",
        ")\n",
        "\n",
        "# a snapshot of the first 5 data points\n",
        "titanic_df.head()\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pclass</th>\n",
              "      <th>survived</th>\n",
              "      <th>name</th>\n",
              "      <th>sex</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>ticket</th>\n",
              "      <th>fare</th>\n",
              "      <th>cabin</th>\n",
              "      <th>embarked</th>\n",
              "      <th>boat</th>\n",
              "      <th>body</th>\n",
              "      <th>home.dest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Allen, Miss. Elisabeth Walton</td>\n",
              "      <td>female</td>\n",
              "      <td>29.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24160</td>\n",
              "      <td>211.3375</td>\n",
              "      <td>B5</td>\n",
              "      <td>S</td>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>St Louis, MO</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Allison, Master. Hudson Trevor</td>\n",
              "      <td>male</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>113781</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>C22 C26</td>\n",
              "      <td>S</td>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Montreal, PQ / Chesterville, ON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Allison, Miss. Helen Loraine</td>\n",
              "      <td>female</td>\n",
              "      <td>2.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>113781</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>C22 C26</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Montreal, PQ / Chesterville, ON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Allison, Mr. Hudson Joshua Creighton</td>\n",
              "      <td>male</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>113781</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>C22 C26</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "      <td>135.0</td>\n",
              "      <td>Montreal, PQ / Chesterville, ON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Allison, Mrs. Hudson J C (Bessie Waldo Daniels)</td>\n",
              "      <td>female</td>\n",
              "      <td>25.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>113781</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>C22 C26</td>\n",
              "      <td>S</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Montreal, PQ / Chesterville, ON</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pclass  survived  ...   body                        home.dest\n",
              "0       1         1  ...    NaN                     St Louis, MO\n",
              "1       1         1  ...    NaN  Montreal, PQ / Chesterville, ON\n",
              "2       1         0  ...    NaN  Montreal, PQ / Chesterville, ON\n",
              "3       1         0  ...  135.0  Montreal, PQ / Chesterville, ON\n",
              "4       1         0  ...    NaN  Montreal, PQ / Chesterville, ON\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yj88WmCmavlf"
      },
      "source": [
        "**The meaning of the different columns (features) is as follows**:\n",
        "\n",
        "<ol>\n",
        "\n",
        "  <li> <b>pclass</b>: Passenger class (1 = first; 2 = second; 3 = third) </li>\n",
        "  <li> <b>survived</b>: Survived? (0 = no; 1 = yes) </li>\n",
        "  <li> <b>name</b>: Name </li>\n",
        "  <li> <b>sex</b>: Sex </li>\n",
        "  <li> <b>age</b>: Age </li>\n",
        "  <li> <b>sibsp</b>: Number of brothers, sisters, or spouses onboard </li>\n",
        "  <li> <b>parch</b>: Number of parents or children onboard </li>\n",
        "  <li> <b>ticket</b>: Ticket number </li>\n",
        "  <li> <b>fare</b>: Passenger fare </li>\n",
        "  <li> <b>cabin</b>: Cabin number </li>\n",
        "  <li> <b>embarked</b>: Port of embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) </li>\n",
        "  <li> <b>boat</b>: Lifeboat (if the passenger survived) </li>\n",
        "  <li> <b>body</b>: Body number (if the passenger did not survive and his body was found) </li>\n",
        "  <li> <b>home.dest</b>: the passenger's destination </li>\n",
        " </ol>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u2ed5fozqjce"
      },
      "source": [
        "## Data pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "__vcZhPnavlg"
      },
      "source": [
        "### Feature selection\n",
        "Some features are not relevant to the task, for example:\n",
        "<ol>\n",
        "  <li> <b>name</b>: Name </li>\n",
        "  <li> <b>ticket</b>: Ticket number </li>\n",
        "  <li> <b>cabin</b>: Cabin number </li>\n",
        "  <li> <b>home.dest</b>: Passenger's destination </li>\n",
        " </ol>\n",
        " \n",
        "Other features give away the label to be predicted and including them would be cheating:\n",
        "<ol>\n",
        "  <li> <b>boat</b>: Lifeboat (if the passenger survived) </li>\n",
        "  <li> <b>body</b>: Body number (if the passenger did not survive and his body was found) </li>\n",
        " </ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JJ0--SDpavlg",
        "outputId": "b016c23b-0994-4cf0-f54b-0ad1de6ef90d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "titanic_preprocess_df = pd.read_csv(\n",
        "    'https://github.com/afansi/winterschool18/blob/master/titanic_prepocess.csv?raw=true', \n",
        "    sep=',', \n",
        "    index_col=None\n",
        ")\n",
        "\n",
        "titanic_preprocess_df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>survived</th>\n",
              "      <th>pclass_1</th>\n",
              "      <th>pclass_2</th>\n",
              "      <th>pclass_3</th>\n",
              "      <th>sex_female</th>\n",
              "      <th>sex_male</th>\n",
              "      <th>age</th>\n",
              "      <th>sibsp</th>\n",
              "      <th>parch</th>\n",
              "      <th>fare</th>\n",
              "      <th>embarked_C</th>\n",
              "      <th>embarked_Q</th>\n",
              "      <th>embarked_S</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>29.0000</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>211.3375</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>30.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>25.0000</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>151.5500</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   survived  pclass_1  pclass_2  ...  embarked_C  embarked_Q  embarked_S\n",
              "0         1         1         0  ...           0           0           1\n",
              "1         1         1         0  ...           0           0           1\n",
              "2         0         1         0  ...           0           0           1\n",
              "3         0         1         0  ...           0           0           1\n",
              "4         0         1         0  ...           0           0           1\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MckYm0M_xhR",
        "colab_type": "text"
      },
      "source": [
        " ### Feature encoding\n",
        " \n",
        "Some features are **categorical variables**, which means that they can take a finite number of values.\n",
        " <ol>\n",
        "  <li> <b>pclass</b>: Passenger Class </li>\n",
        "  <li> <b>sex</b>: Sex </li>\n",
        "  <li> <b>embarked</b>: Port of embarkation </li>\n",
        " </ol>\n",
        "\n",
        "To process categorical variables, we need to encode them in a way that does not imply an arbitrary order such as using natural numbers (e.g., 1, 2, 3). <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">One-hot encoding</a> is a way to achieve it. We can download the pre-processed dataset at the following address: https://github.com/afansi/winterschool18/blob/master/titanic_prepocess.csv?raw=true.\n",
        "<br>The meaning of the encoded variables is as follows:\n",
        "\n",
        "<ol>\n",
        "  <li> <b>survived</b>: Survived? (0 = no; 1 = yes) </li>\n",
        "  <li> <b>pclass_1</b>: (1 if passenger in first class; 0 if not) </li>\n",
        "  <li> <b>pclass_2</b>: (1 if passenger in second class; 0 if not) </li>\n",
        "  <li> <b>pclass_3</b>: (1 if passenger in third class; 0 if not) </li>\n",
        "  <li> <b>sex_female</b>: (1 if passenger is female; 0 if not) </li>\n",
        "  <li> <b>sex_male</b>: (1 if passenger is male; 0 if not) </li>\n",
        "  <li> <b>age</b>: Age </li>\n",
        "  <li> <b>sibsp</b>: Number of brothers, sisters, or spouses onboard </li>\n",
        "  <li> <b>parch</b>: Number of parents or children onboard </li>\n",
        "  <li> <b>fare</b>: Passenger fare </li>\n",
        "  <li> <b>embarked_C</b>: (1 if Port of embarkation = Cherbourg (C); 0 otherwise) </li> \n",
        "  <li> <b>embarked_Q</b>: (1 if Port of embarkation = Queenstown (Q); 0 otherwise) </li> \n",
        "  <li> <b>embarked_S</b>: (1 if Port of embarkation = Southampton (S); 0 otherwise)</li> \n",
        " </ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QJcs6PUTavlm"
      },
      "source": [
        "## Train / validation / test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bjbgvffmavlo"
      },
      "source": [
        "At this point, we need to divide the dataset into three subsets:\n",
        "\n",
        "<ol>\n",
        "<li> <b> Train</b> (usually 60% of the dataset): used to train the classification model. </li>   \n",
        "<li> <b> Validation</b> (generally 20% of the dataset): used to evaluate hyper-parameters on held-out data. </li>   \n",
        "<li> <b> Test</b> (usually 20% of the dataset): used to evaluate the generalization performance of the chosen model on held-out data. </li>\n",
        "</ol>\n",
        "\n",
        "We use the <a href=\"https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.split.html\">numpy.split function</a> to separate our dataset into subsets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GBmL8VBOavlo",
        "colab": {}
      },
      "source": [
        "train, validate, test = np.split(\n",
        "    titanic_preprocess_df.sample(frac=1, random_state=seed), \n",
        "    [int(.6*len(titanic_preprocess_df)), int(.8*len(titanic_preprocess_df))])\n",
        "\n",
        "# Remove the label column from X and create a label vectors.\n",
        "X_train = train.drop(['survived'], axis=1).values\n",
        "y_train = train['survived'].values\n",
        "\n",
        "X_val = validate.drop(['survived'], axis=1).values # To complete.\n",
        "y_val = validate['survived'].values # To complete.\n",
        "\n",
        "X_test = test.drop(['survived'], axis=1).values # To complete.\n",
        "y_test = test['survived'].values # To complete.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wv74TbIWavlr"
      },
      "source": [
        "## Datasets in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9_LJtG-Xavlt"
      },
      "source": [
        "We will use the subclass <b><a href=\"http://pytorch.org/docs/master/data.html#\"> `torch.utils.data.TensorDataset`</a> </b> to manipulate together the features and targets of a dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1JtT4tV7avlt",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "train_dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n",
        "\n",
        "val_dataset = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long()) # To complete.\n",
        "\n",
        "test_dataset = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).long()) # To complete.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "obEPHnlTavkc"
      },
      "source": [
        "# How to define the learning algorithm?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qhN5GL6Gavks"
      },
      "source": [
        "A multilayer perceptron (MLP) is a simple computational graph composed of \"hidden layers,\" which are defined by two modules: a *linear transformation* followed by a *non-linearity*. The result of a hidden layer is a vector called *a distributed representation* where each component is associated with a hidden unit.\n",
        "\n",
        "To train this model, we need to define:\n",
        "<ul>\n",
        "<li>the network architecture by choosing the non-linear function and the number of hidden units per layer, </li>\n",
        "<li>the cost function and optimizer. </li>\n",
        "</ul>\n",
        "\n",
        "To solve our task, we will use a MLP with the following properties:\n",
        " <ul>\n",
        " <li> the input dimension of the model is 12,</li>\n",
        " <li> the output dimension of the model is 2,</li>\n",
        " <li> the first dimension of the output is the probability of death and the second dimension is the probability of survival,</li>\n",
        "  <li> the number of hidden layers is 3, </li>\n",
        " <li> the dimensions of the hidden layers are 20, 40, 20 respectively, </li>\n",
        " <li> the activation function is a ReLu for all hidden layers. </li>\n",
        " </ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "701t0e-ravkr"
      },
      "source": [
        "## How to define a model in PyTorch?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m4F5cyijavkv"
      },
      "source": [
        "The <a href=\"https://pytorch.org/docs/stable/nn.html\">PyTorch NN package</a> contains many useful classes for creating computation graphs.\n",
        "<ul>\n",
        "<li> The class <a href=\"http://pytorch.org/docs/master/nn.html#module\">torch.nn.Module</a>: \n",
        "any new module must inherit from this class or its descendants (subclasses).\n",
        "</li>   \n",
        "<li> The `forward` method:  any class defining a module must implement the `forward(...)` method, which defines the transformation of inputs to outputs.</li>  \n",
        "<li> The class <a href=\"http://pytorch.org/docs/master/nn.html#torch.nn.Linear\">`torch.nn.Linear(in_features, out_features)`</a>: this class implements a linear transformation. By default, it takes two parameters: \n",
        "    <ul>\n",
        "    <li>`in_features`: the size of the data at the input of the module. </li>\n",
        "    <li>`out_features`: the size of the data at the output of the module. </li>\n",
        "    </ul>\n",
        "</li>\n",
        "<li> The module <a href=\"http://pytorch.org/docs/master/nn.html#torch-nn-functional\">`torch.nn.functional`</a>: \n",
        "it defines a set of functions that can be applied directly to any tensor. As examples, we have:\n",
        "    <ul>\n",
        "    <li> non-linear functions: sigmoid(...), tanh(...), relu(...), etc...</li> \n",
        "    <li> cost functions: mse_loss(...), nll(...., cross_entropy(...), etc ... </li> \n",
        "    <li> regularization functions: droupout(...), etc ... </li> \n",
        "    <li> ...</li> \n",
        "    </ul>\n",
        "</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tscha6S-KIBB",
        "colab_type": "text"
      },
      "source": [
        "You need to complete the following methods:\n",
        "<ul>\n",
        "<li>The `__init__` method that defines the layers. </li>\n",
        "<li>The `forward(input)` method that returns the `output`. </li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8NyQGwC-avkw",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "seed = 1234\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xR5eBfIbavk0",
        "colab": {}
      },
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(12, 20)\n",
        "        self.fc2 = nn.Linear(20, 40)\n",
        "        self.fc3 = nn.Linear(40, 20)\n",
        "        self.fc4 = nn.Linear(20, 2) # To complete.\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        out = self.fc4(x) # To complete.\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OvLnHRZ5avk2"
      },
      "source": [
        "## Making predictions with a neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uEXgJMDDavk3"
      },
      "source": [
        "Now, we are ready to test our neural network on randomly selected data.\n",
        "\n",
        "In PyTorch, a model has two different modes:\n",
        "    <ul>\n",
        "    <li> <b>train</b>: used during training, </li>\n",
        "    <li> <b>eval</b>: used during inference for model evaluation. </li>\n",
        "    </ul>\n",
        "The distinction is important since some modules behave differently according to this mode.\n",
        "We will use the <b>eval</b> mode in this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gzcABMezavk6",
        "outputId": "d7b612a6-59ec-4110-a73e-02e4754d9c89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "# Model definition\n",
        "neural_net = NeuralNet()\n",
        "neural_net = neural_net.to(device)\n",
        "\n",
        "# Eval mode activation\n",
        "neural_net = neural_net.eval()\n",
        "\n",
        "# Select the first 5 data points\n",
        "data, target = val_dataset[0:5]\n",
        "data = data.to(device)\n",
        "target = target.to(device)\n",
        "\n",
        "# Forward propagation of the data through the model\n",
        "output = neural_net(data)   # equivalent to neural_net.forward(data)\n",
        "\n",
        "# Convert the logits into probabilities with softmax function\n",
        "output_proba = F.softmax(output, dim=1) # To complete.\n",
        "\n",
        "# Printing the probability\n",
        "print(output_proba)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.5093, 0.4907],\n",
            "        [0.5069, 0.4931],\n",
            "        [0.5129, 0.4871],\n",
            "        [0.5022, 0.4978],\n",
            "        [0.4427, 0.5573]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fVep0BElavlS"
      },
      "source": [
        "The rows define the output of the network, in terms of probabilities over two classes: <b>deceased</b> (first column) or <b>survived</b> (second column), for each of the five input data points. Let us take the label with maximum probability as the predicted label and compare it to the correct label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_jV4No36qjdU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "85b8e811-7fc8-401c-8bc9-63f03d9d1e2c"
      },
      "source": [
        "# Printing predictions (class with the highest probability)\n",
        "_, prediction = torch.max(output_proba, dim=1)\n",
        "\n",
        "print('Model prediction')\n",
        "print(prediction)\n",
        "\n",
        "# Printing the real labels\n",
        "print(\"Actual data\")\n",
        "print(target)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model prediction\n",
            "tensor([0, 0, 0, 0, 1], device='cuda:0')\n",
            "Actual data\n",
            "tensor([0, 0, 0, 0, 0], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SEIIjqOuqjdc"
      },
      "source": [
        "**Questions**\n",
        "\n",
        "1.   What would be a good way to measure the model performances?\n",
        "2.   How does our model perform?\n",
        "3.   Considering that the model is not trained on the dataset, do you see any problem with your selected measure?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTTlBikYwb-w",
        "colab_type": "text"
      },
      "source": [
        "... # To complete.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0uySA2TCavmD"
      },
      "source": [
        "## Define the cost function and optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EkoobCLMavmE"
      },
      "source": [
        "### Cost function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qkX7uSXQavmF"
      },
      "source": [
        "We define the cost function according to the task we want to achieve.\n",
        "\n",
        "PyTorch offers <a href=\"https://pytorch.org/docs/stable/nn.html#loss-functions\">many ready-to-use cost functions</a>.\n",
        "\n",
        "For classification problems, the usual cost function is <b>cross-entropy</b>, and this is the one we will use in this tutorial. In PyTorch, it is defined by the function <a href=\"https://pytorch.org/docs/master/nn.functional.html#cross-entropy\">`torch.nn.functional.cross_entropy`</a>.  Cross entropy allows comparing a $p$ distribution with a reference distribution $t$. It attains its minimum when $t=p$. Its formula for calculating it between the prediction and the target is: $-\\sum_j t_{ij} \\log(p_{ij})$ where $p$ is the prediction, $t$ the target, $i$ the examples and $j$ the classes of the target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FHnfYeS5avmF",
        "colab": {}
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def cost_function(prediction, target):\n",
        "    loss = F.cross_entropy(prediction, target) # To complete.\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Vsx_cv9Wqjdj"
      },
      "source": [
        "### Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0hcZaIKtavmH"
      },
      "source": [
        "In Pytorch, thanks to the automatic differentiation mechanism <a href=\"http://pytorch.org/docs/master/notes/autograd.html\">Autograd</a>, it is possible to automatically calculate the gradient of the cost function and backpropagate it through the computational graph.\n",
        "\n",
        "To do this, we only have to call the method `backward()` on the variable returned by the cost function, e.g., with\n",
        "\n",
        "`loss = cost_function(....)` <br>\n",
        "`loss.backward()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8YNo_ymYavmH"
      },
      "source": [
        "### Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Y4AlX9TwavmH"
      },
      "source": [
        "PyTorch provides a <a href=\"http://pytorch.org/docs/master/optim.html#algorithms\">set of optimization methods (`torch.optim`)</a> commonly used by the deep learning community. These methods include the following: \n",
        "<ul>\n",
        "<li><b>SGD</b> (Stochastic Gradient Descent) <a href=\"http://pytorch.org/docs/master/optim.html#torch.optim.SGD\">`torch.optim.SGD(net.parameters(), lr=learning_rate)`</a></li>\n",
        "<li><b>Adam</b> (Adaptive Moment Estimation): a variant of the gradient descent method in which the learning rate is adjusted for each parameter. This adjustment is based on the estimation of the first and second moments of the gradients. This optimizer has demonstrated excellent performance compared to SGD on many benchmarks. </li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Uam-a0_0qjdl"
      },
      "source": [
        "To be able to use an optimizer in PyTorch, we must instantiate it by passing the following elements:\n",
        "<ul>\n",
        "<li><b>The parameters of the model</b>: these are obtained using the method <b>parameters()</b> on the instantiated model. </li>\n",
        "<li><b>The learning rate (lr)</b>: this is the learning rate to be used to update parameters during the optimization process. </li>\n",
        "<li>There may be other parameters specific to the chosen optimizer.</li>\n",
        "</ul>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jt6_Qr6ravmI"
      },
      "source": [
        "PyTorch offers a simplified interface to interact with any optimizer:\n",
        "<ul>\n",
        "<li><b>zero_grad()</b>: Allows to reinitialize the gradients to zero at the beginning of an optimization step.</li>\n",
        "<li><b>step()</b>: Allows to perform an optimization step after a gradient backpropagation step.</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fZ-lKExqavmI"
      },
      "source": [
        "We will use Adam with a lr of 0.001."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WDMOziJTavmI",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(neural_net.parameters(), lr=0.001) # To complete.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OnFOAfdGqjdr"
      },
      "source": [
        "# How to train a model?\n",
        "First, we need some definitions:\n",
        "<ol>\n",
        "<li>\n",
        "<b>Epoch</b>: a complete pass over the entire training dataset.\n",
        "</li>\n",
        "<li>\n",
        "<b>Iteration</b>: an update of the model parameters. Many iterations can occur before the end of an epoch.\n",
        "</li>\n",
        "<li>\n",
        "<b>Mini-batch</b>: A subset of training data used to estimate the average of gradients. In other words, at each iteration, a mini-batch is used. \n",
        "</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LLXjNiDTavmK"
      },
      "source": [
        "## Creating the mini-batches\n",
        "PyTorch offers a utility called <b><a href=\"http://pytorch.org/docs/master/data.html\"> torch.utils.data.DataLoader </a></b> to load any dataset and automatically split it into mini-batches. During training, the data presented to the network should appear in a different order from one epoch to another. We will prepare the `DataLoader` for our three datasets (training, validation, and test)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RGoQZSdqavmM",
        "colab": {}
      },
      "source": [
        "train_batch_size = 32  # number of data in a training batch.\n",
        "eval_batch_size = 32   # number of data in an batch.\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
        "\n",
        "val_loader   = torch.utils.data.DataLoader(val_dataset, batch_size=eval_batch_size, shuffle=False) # To complete.\n",
        "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=eval_batch_size, shuffle=False) # To complete.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ia3ai-GvavmP"
      },
      "source": [
        "## Simple training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v9wNZrTnavmQ"
      },
      "source": [
        "Here we define our training procedure for an epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZyK9xCsZavmR",
        "colab": {}
      },
      "source": [
        "def train(epoch, model, train_loader, optimizer, device):\n",
        "    \n",
        "    # activate the training mode\n",
        "    model.train()\n",
        "    \n",
        "    torch.set_grad_enabled(True)\n",
        "    \n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    # iteration over the mini-batches\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        \n",
        "        # transfer the data on the chosen device\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # reinitialize the gradients to zero\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # forward propagation on the data\n",
        "        prediction = model(data)\n",
        "        \n",
        "        # compute the cost function w.r.t. the targets\n",
        "        loss = cost_function(prediction, target)\n",
        "        \n",
        "        # execute the backpropagation\n",
        "        loss.backward()\n",
        "        \n",
        "        # execute an optimization step\n",
        "        optimizer.step()\n",
        "        \n",
        "        # accumulate the loss\n",
        "        total_loss += loss.item()*len(data)\n",
        "        \n",
        "        # compute the number of correct predictions\n",
        "        _, pred_classes = torch.max(prediction, dim=1)        \n",
        "        correct += pred_classes.eq(target.view_as(pred_classes)).sum().item()\n",
        "         \n",
        "        \n",
        "    # compute the average cost per epoch\n",
        "    mean_loss = total_loss/len(train_loader.dataset)\n",
        "    \n",
        "    # compute the accuracy\n",
        "    acc = correct / len(train_loader.dataset)\n",
        "        \n",
        "    print('Train Epoch: {}   Avg_Loss: {:.5f}   Acc: {}/{} ({:.3f}%)'.format(\n",
        "        epoch, mean_loss, correct, len(train_loader.dataset),\n",
        "        100. * acc))   \n",
        "    \n",
        "    # return the average loss and the accuracy\n",
        "    return mean_loss, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PxG666rmavmU"
      },
      "source": [
        "## Evaluation procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vGexbWaHavmU"
      },
      "source": [
        "Here we define our model evaluation procedure.\n",
        "<br/>\n",
        "In addition to switching the model to **eval** mode, it is essential to disable the gradient calculation. \n",
        "<br/>\n",
        "To do this, PyTorch offers a set of context managers to <a href=\"https://pytorch.org/docs/0.4.0/torch.html#locally-disabling-gradient-computation\">locally disable/enable gradient calculation </a>:\n",
        "<ol>\n",
        "<li>\n",
        "`torch.no_grad()`: disable gradient calculation.\n",
        "</li>\n",
        "<li>\n",
        "`torch.enable_grad()`: enable gradient calculation.\n",
        "</li>\n",
        "<li>\n",
        "`torch.set_grad_enabled(bool)`: enable/disable gradient calculation.\n",
        "</li>\n",
        "</ol>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8gQj9W5LavmU",
        "colab": {}
      },
      "source": [
        "def evaluate(model, eval_loader, device):\n",
        "    \n",
        "    # activate the evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        # iterate over the batches\n",
        "        for batch_idx, (data, target) in enumerate(eval_loader):\n",
        "\n",
        "            # transfer the data on the chosen device\n",
        "            data, target = data.to(device), target.to(device)\n",
        "\n",
        "            # forward propagation on the data\n",
        "            prediction = model(data)\n",
        "\n",
        "            # compute the cost function w.r.t. the targets\n",
        "            loss = cost_function(prediction, target)           \n",
        "\n",
        "\n",
        "            # accumulate the loss\n",
        "            total_loss += loss.item()*len(data)\n",
        "\n",
        "            # compute the number of correct predictions en sortie)\n",
        "            _, pred_classes = torch.max(prediction, dim=1) \n",
        "            correct += pred_classes.eq(target.view_as(pred_classes)).sum().item()         \n",
        "          \n",
        "    \n",
        "    # compute the average cost per epoch\n",
        "    mean_loss = total_loss/len(eval_loader.dataset)\n",
        "    \n",
        "    # compute the accuracy\n",
        "    acc = correct / len(eval_loader.dataset)\n",
        "        \n",
        "    print('Eval:  Avg_Loss: {:.5f}   Acc: {}/{} ({:.3f}%)'.format(\n",
        "        mean_loss, correct, len(eval_loader.dataset),\n",
        "        100. * acc)) \n",
        "    \n",
        "    # return the average loss and the accuracy\n",
        "    return mean_loss, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fMUyZNxdavmW"
      },
      "source": [
        "## Checkpointing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lQLklQXAavmW"
      },
      "source": [
        "For training phases that require much time, it is recommended to save periodically the model parameters. This step is commonly referred to as <b>checkpointing</b>.\n",
        "\n",
        "PyTorch offers <a href=\"http://pytorch.org/docs/master/notes/serialization.html\">a simple mechanism</a> to perform this operation. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ld-Y2gF-avmX"
      },
      "source": [
        "We implement two methods here:\n",
        "<ul>\n",
        "<li> the first one for <b> saving </b> a model,</li>\n",
        "<li> the second for <b> loading </b> a model checkpoint. </li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dMmNpma2avmX",
        "colab": {}
      },
      "source": [
        "def save_model(epoch, model, path='./'):\n",
        "    \n",
        "    # creating the file name indexed by the epoch value\n",
        "    filename = path + 'neural_network_{}.pt'.format(epoch)\n",
        "    \n",
        "    # saving the model parameters\n",
        "    torch.save(model.state_dict(), filename)\n",
        "    \n",
        "    \n",
        "    return model\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2ZptgqQRavmZ",
        "colab": {}
      },
      "source": [
        "def load_model(epoch, model, path='./'):\n",
        "    \n",
        "    # creating the file name indexed by the epoch value\n",
        "    filename = path + 'neural_network_{}.pt'.format(epoch)\n",
        "    \n",
        "    # loading the parameters of the saved model\n",
        "    model.load_state_dict(torch.load(filename))\n",
        "    \n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ve8sOocWavma"
      },
      "source": [
        "It is also possible to save the status of the optimizer in PyTorch, which is very important when we want to resume training the model from a given backup. For more information, please consult <a href='https://discuss.pytorch.org/t/saving-and-loading-a-model-in-pytorch/2610/3'>the following URL</a>. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8lcAP8-1avma"
      },
      "source": [
        "## Putting everything together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "keMpyePsavmb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7ddea4b7-aee4-47c9-83a1-7d393e29f2f2"
      },
      "source": [
        "# maximum number of epoch\n",
        "numEpochs = 200\n",
        "\n",
        "# Saving frequency\n",
        "checkpoint_freq = 10\n",
        "\n",
        "# Directory for data backup\n",
        "path = './'\n",
        "\n",
        "# Accumulators of average costs obtained per epoch\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "# Performance accumulators per epoch\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "\n",
        "# Model definition\n",
        "neural_net = NeuralNet()\n",
        "\n",
        "# Load the model on the chosen device\n",
        "neural_net = neural_net.to(device)\n",
        "\n",
        "# Optimizer definition\n",
        "optimizer = optim.Adam(neural_net.parameters(), lr=0.001) \n",
        "# optimizer = optim.SGD(neural_net.parameters(), lr=0.001) \n",
        "\n",
        "\n",
        "# Learning loop\n",
        "for epoch in range(1, numEpochs + 1):\n",
        "    \n",
        "    # train the model with the train dataset\n",
        "    train_loss, train_acc = train(epoch, neural_net, train_loader, optimizer, device)   \n",
        "    \n",
        "    # evaluate the model with the validation dataset\n",
        "    val_loss, val_acc = evaluate(neural_net, val_loader, device)       \n",
        "    \n",
        "    # Save the costs obtained\n",
        "    train_losses.append(train_loss)    \n",
        "    val_losses.append(val_loss)\n",
        "    \n",
        "    # Save the performances\n",
        "    train_accuracies.append(train_acc)    \n",
        "    val_accuracies.append(val_acc)\n",
        "    \n",
        "    # Checkpoint\n",
        "    if epoch % checkpoint_freq ==0:\n",
        "        save_model(epoch, neural_net, path)\n",
        "\n",
        "# Save the model at the end of the training\n",
        "save_model(numEpochs, neural_net, path)\n",
        "    \n",
        "print(\"\\n\\n\\nOptimization ended.\\n\")    \n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1   Avg_Loss: 0.82669   Acc: 275/625 (44.000%)\n",
            "Eval:  Avg_Loss: 0.69597   Acc: 99/209 (47.368%)\n",
            "Train Epoch: 2   Avg_Loss: 0.66111   Acc: 397/625 (63.520%)\n",
            "Eval:  Avg_Loss: 0.65175   Acc: 144/209 (68.900%)\n",
            "Train Epoch: 3   Avg_Loss: 0.63141   Acc: 421/625 (67.360%)\n",
            "Eval:  Avg_Loss: 0.62352   Acc: 145/209 (69.378%)\n",
            "Train Epoch: 4   Avg_Loss: 0.61599   Acc: 423/625 (67.680%)\n",
            "Eval:  Avg_Loss: 0.61681   Acc: 142/209 (67.943%)\n",
            "Train Epoch: 5   Avg_Loss: 0.61261   Acc: 431/625 (68.960%)\n",
            "Eval:  Avg_Loss: 0.60453   Acc: 142/209 (67.943%)\n",
            "Train Epoch: 6   Avg_Loss: 0.61117   Acc: 426/625 (68.160%)\n",
            "Eval:  Avg_Loss: 0.60108   Acc: 144/209 (68.900%)\n",
            "Train Epoch: 7   Avg_Loss: 0.60210   Acc: 427/625 (68.320%)\n",
            "Eval:  Avg_Loss: 0.59878   Acc: 143/209 (68.421%)\n",
            "Train Epoch: 8   Avg_Loss: 0.60072   Acc: 427/625 (68.320%)\n",
            "Eval:  Avg_Loss: 0.59606   Acc: 144/209 (68.900%)\n",
            "Train Epoch: 9   Avg_Loss: 0.59455   Acc: 429/625 (68.640%)\n",
            "Eval:  Avg_Loss: 0.58267   Acc: 145/209 (69.378%)\n",
            "Train Epoch: 10   Avg_Loss: 0.58700   Acc: 429/625 (68.640%)\n",
            "Eval:  Avg_Loss: 0.57752   Acc: 146/209 (69.856%)\n",
            "Train Epoch: 11   Avg_Loss: 0.57737   Acc: 435/625 (69.600%)\n",
            "Eval:  Avg_Loss: 0.56965   Acc: 146/209 (69.856%)\n",
            "Train Epoch: 12   Avg_Loss: 0.56613   Acc: 445/625 (71.200%)\n",
            "Eval:  Avg_Loss: 0.56516   Acc: 146/209 (69.856%)\n",
            "Train Epoch: 13   Avg_Loss: 0.55887   Acc: 454/625 (72.640%)\n",
            "Eval:  Avg_Loss: 0.55601   Acc: 151/209 (72.249%)\n",
            "Train Epoch: 14   Avg_Loss: 0.55906   Acc: 456/625 (72.960%)\n",
            "Eval:  Avg_Loss: 0.56250   Acc: 147/209 (70.335%)\n",
            "Train Epoch: 15   Avg_Loss: 0.54458   Acc: 462/625 (73.920%)\n",
            "Eval:  Avg_Loss: 0.53504   Acc: 152/209 (72.727%)\n",
            "Train Epoch: 16   Avg_Loss: 0.52877   Acc: 470/625 (75.200%)\n",
            "Eval:  Avg_Loss: 0.52433   Acc: 156/209 (74.641%)\n",
            "Train Epoch: 17   Avg_Loss: 0.50872   Acc: 479/625 (76.640%)\n",
            "Eval:  Avg_Loss: 0.52399   Acc: 157/209 (75.120%)\n",
            "Train Epoch: 18   Avg_Loss: 0.49639   Acc: 485/625 (77.600%)\n",
            "Eval:  Avg_Loss: 0.50610   Acc: 161/209 (77.033%)\n",
            "Train Epoch: 19   Avg_Loss: 0.48659   Acc: 486/625 (77.760%)\n",
            "Eval:  Avg_Loss: 0.51451   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 20   Avg_Loss: 0.49765   Acc: 478/625 (76.480%)\n",
            "Eval:  Avg_Loss: 0.50342   Acc: 163/209 (77.990%)\n",
            "Train Epoch: 21   Avg_Loss: 0.46487   Acc: 497/625 (79.520%)\n",
            "Eval:  Avg_Loss: 0.50306   Acc: 160/209 (76.555%)\n",
            "Train Epoch: 22   Avg_Loss: 0.48441   Acc: 483/625 (77.280%)\n",
            "Eval:  Avg_Loss: 0.50032   Acc: 162/209 (77.512%)\n",
            "Train Epoch: 23   Avg_Loss: 0.47035   Acc: 483/625 (77.280%)\n",
            "Eval:  Avg_Loss: 0.50619   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 24   Avg_Loss: 0.46559   Acc: 491/625 (78.560%)\n",
            "Eval:  Avg_Loss: 0.50215   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 25   Avg_Loss: 0.46135   Acc: 492/625 (78.720%)\n",
            "Eval:  Avg_Loss: 0.49361   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 26   Avg_Loss: 0.44812   Acc: 498/625 (79.680%)\n",
            "Eval:  Avg_Loss: 0.49646   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 27   Avg_Loss: 0.44118   Acc: 500/625 (80.000%)\n",
            "Eval:  Avg_Loss: 0.50127   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 28   Avg_Loss: 0.43890   Acc: 507/625 (81.120%)\n",
            "Eval:  Avg_Loss: 0.50065   Acc: 163/209 (77.990%)\n",
            "Train Epoch: 29   Avg_Loss: 0.43732   Acc: 502/625 (80.320%)\n",
            "Eval:  Avg_Loss: 0.51434   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 30   Avg_Loss: 0.43915   Acc: 504/625 (80.640%)\n",
            "Eval:  Avg_Loss: 0.50415   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 31   Avg_Loss: 0.44063   Acc: 503/625 (80.480%)\n",
            "Eval:  Avg_Loss: 0.51910   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 32   Avg_Loss: 0.45316   Acc: 502/625 (80.320%)\n",
            "Eval:  Avg_Loss: 0.50938   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 33   Avg_Loss: 0.46385   Acc: 501/625 (80.160%)\n",
            "Eval:  Avg_Loss: 0.50409   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 34   Avg_Loss: 0.44505   Acc: 500/625 (80.000%)\n",
            "Eval:  Avg_Loss: 0.49981   Acc: 163/209 (77.990%)\n",
            "Train Epoch: 35   Avg_Loss: 0.42757   Acc: 510/625 (81.600%)\n",
            "Eval:  Avg_Loss: 0.50079   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 36   Avg_Loss: 0.42489   Acc: 510/625 (81.600%)\n",
            "Eval:  Avg_Loss: 0.50493   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 37   Avg_Loss: 0.42569   Acc: 504/625 (80.640%)\n",
            "Eval:  Avg_Loss: 0.51543   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 38   Avg_Loss: 0.42767   Acc: 508/625 (81.280%)\n",
            "Eval:  Avg_Loss: 0.52462   Acc: 161/209 (77.033%)\n",
            "Train Epoch: 39   Avg_Loss: 0.43664   Acc: 500/625 (80.000%)\n",
            "Eval:  Avg_Loss: 0.52757   Acc: 161/209 (77.033%)\n",
            "Train Epoch: 40   Avg_Loss: 0.42918   Acc: 505/625 (80.800%)\n",
            "Eval:  Avg_Loss: 0.51325   Acc: 163/209 (77.990%)\n",
            "Train Epoch: 41   Avg_Loss: 0.42343   Acc: 513/625 (82.080%)\n",
            "Eval:  Avg_Loss: 0.52806   Acc: 158/209 (75.598%)\n",
            "Train Epoch: 42   Avg_Loss: 0.42787   Acc: 509/625 (81.440%)\n",
            "Eval:  Avg_Loss: 0.51280   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 43   Avg_Loss: 0.42026   Acc: 509/625 (81.440%)\n",
            "Eval:  Avg_Loss: 0.52326   Acc: 165/209 (78.947%)\n",
            "Train Epoch: 44   Avg_Loss: 0.41762   Acc: 512/625 (81.920%)\n",
            "Eval:  Avg_Loss: 0.50781   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 45   Avg_Loss: 0.41870   Acc: 505/625 (80.800%)\n",
            "Eval:  Avg_Loss: 0.51903   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 46   Avg_Loss: 0.42444   Acc: 514/625 (82.240%)\n",
            "Eval:  Avg_Loss: 0.50020   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 47   Avg_Loss: 0.42592   Acc: 510/625 (81.600%)\n",
            "Eval:  Avg_Loss: 0.51958   Acc: 163/209 (77.990%)\n",
            "Train Epoch: 48   Avg_Loss: 0.41998   Acc: 504/625 (80.640%)\n",
            "Eval:  Avg_Loss: 0.51545   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 49   Avg_Loss: 0.42676   Acc: 511/625 (81.760%)\n",
            "Eval:  Avg_Loss: 0.50748   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 50   Avg_Loss: 0.41644   Acc: 509/625 (81.440%)\n",
            "Eval:  Avg_Loss: 0.49979   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 51   Avg_Loss: 0.40603   Acc: 514/625 (82.240%)\n",
            "Eval:  Avg_Loss: 0.52157   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 52   Avg_Loss: 0.41039   Acc: 515/625 (82.400%)\n",
            "Eval:  Avg_Loss: 0.51693   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 53   Avg_Loss: 0.40603   Acc: 514/625 (82.240%)\n",
            "Eval:  Avg_Loss: 0.51723   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 54   Avg_Loss: 0.40778   Acc: 510/625 (81.600%)\n",
            "Eval:  Avg_Loss: 0.50862   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 55   Avg_Loss: 0.40526   Acc: 512/625 (81.920%)\n",
            "Eval:  Avg_Loss: 0.51535   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 56   Avg_Loss: 0.40185   Acc: 512/625 (81.920%)\n",
            "Eval:  Avg_Loss: 0.52275   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 57   Avg_Loss: 0.40530   Acc: 513/625 (82.080%)\n",
            "Eval:  Avg_Loss: 0.51879   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 58   Avg_Loss: 0.40696   Acc: 511/625 (81.760%)\n",
            "Eval:  Avg_Loss: 0.51691   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 59   Avg_Loss: 0.40390   Acc: 516/625 (82.560%)\n",
            "Eval:  Avg_Loss: 0.50265   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 60   Avg_Loss: 0.41310   Acc: 516/625 (82.560%)\n",
            "Eval:  Avg_Loss: 0.52128   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 61   Avg_Loss: 0.40400   Acc: 519/625 (83.040%)\n",
            "Eval:  Avg_Loss: 0.51773   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 62   Avg_Loss: 0.40380   Acc: 520/625 (83.200%)\n",
            "Eval:  Avg_Loss: 0.51921   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 63   Avg_Loss: 0.40793   Acc: 510/625 (81.600%)\n",
            "Eval:  Avg_Loss: 0.53684   Acc: 163/209 (77.990%)\n",
            "Train Epoch: 64   Avg_Loss: 0.40719   Acc: 509/625 (81.440%)\n",
            "Eval:  Avg_Loss: 0.56113   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 65   Avg_Loss: 0.41137   Acc: 511/625 (81.760%)\n",
            "Eval:  Avg_Loss: 0.50493   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 66   Avg_Loss: 0.40576   Acc: 512/625 (81.920%)\n",
            "Eval:  Avg_Loss: 0.52178   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 67   Avg_Loss: 0.40182   Acc: 512/625 (81.920%)\n",
            "Eval:  Avg_Loss: 0.54088   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 68   Avg_Loss: 0.40665   Acc: 517/625 (82.720%)\n",
            "Eval:  Avg_Loss: 0.52825   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 69   Avg_Loss: 0.39952   Acc: 516/625 (82.560%)\n",
            "Eval:  Avg_Loss: 0.51818   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 70   Avg_Loss: 0.39369   Acc: 516/625 (82.560%)\n",
            "Eval:  Avg_Loss: 0.52185   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 71   Avg_Loss: 0.40292   Acc: 519/625 (83.040%)\n",
            "Eval:  Avg_Loss: 0.53756   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 72   Avg_Loss: 0.39509   Acc: 514/625 (82.240%)\n",
            "Eval:  Avg_Loss: 0.53851   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 73   Avg_Loss: 0.41085   Acc: 506/625 (80.960%)\n",
            "Eval:  Avg_Loss: 0.51058   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 74   Avg_Loss: 0.39671   Acc: 517/625 (82.720%)\n",
            "Eval:  Avg_Loss: 0.50354   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 75   Avg_Loss: 0.39148   Acc: 519/625 (83.040%)\n",
            "Eval:  Avg_Loss: 0.52235   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 76   Avg_Loss: 0.39521   Acc: 516/625 (82.560%)\n",
            "Eval:  Avg_Loss: 0.51323   Acc: 173/209 (82.775%)\n",
            "Train Epoch: 77   Avg_Loss: 0.39041   Acc: 519/625 (83.040%)\n",
            "Eval:  Avg_Loss: 0.53582   Acc: 172/209 (82.297%)\n",
            "Train Epoch: 78   Avg_Loss: 0.39357   Acc: 520/625 (83.200%)\n",
            "Eval:  Avg_Loss: 0.53427   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 79   Avg_Loss: 0.39795   Acc: 512/625 (81.920%)\n",
            "Eval:  Avg_Loss: 0.53290   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 80   Avg_Loss: 0.39800   Acc: 519/625 (83.040%)\n",
            "Eval:  Avg_Loss: 0.55693   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 81   Avg_Loss: 0.42983   Acc: 503/625 (80.480%)\n",
            "Eval:  Avg_Loss: 0.55671   Acc: 164/209 (78.469%)\n",
            "Train Epoch: 82   Avg_Loss: 0.43749   Acc: 491/625 (78.560%)\n",
            "Eval:  Avg_Loss: 0.55337   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 83   Avg_Loss: 0.40582   Acc: 517/625 (82.720%)\n",
            "Eval:  Avg_Loss: 0.55427   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 84   Avg_Loss: 0.40163   Acc: 514/625 (82.240%)\n",
            "Eval:  Avg_Loss: 0.55233   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 85   Avg_Loss: 0.40234   Acc: 514/625 (82.240%)\n",
            "Eval:  Avg_Loss: 0.53004   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 86   Avg_Loss: 0.39289   Acc: 516/625 (82.560%)\n",
            "Eval:  Avg_Loss: 0.54073   Acc: 172/209 (82.297%)\n",
            "Train Epoch: 87   Avg_Loss: 0.39834   Acc: 516/625 (82.560%)\n",
            "Eval:  Avg_Loss: 0.52591   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 88   Avg_Loss: 0.39600   Acc: 511/625 (81.760%)\n",
            "Eval:  Avg_Loss: 0.52622   Acc: 174/209 (83.254%)\n",
            "Train Epoch: 89   Avg_Loss: 0.38880   Acc: 520/625 (83.200%)\n",
            "Eval:  Avg_Loss: 0.52301   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 90   Avg_Loss: 0.38324   Acc: 523/625 (83.680%)\n",
            "Eval:  Avg_Loss: 0.54340   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 91   Avg_Loss: 0.39424   Acc: 515/625 (82.400%)\n",
            "Eval:  Avg_Loss: 0.55621   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 92   Avg_Loss: 0.39279   Acc: 510/625 (81.600%)\n",
            "Eval:  Avg_Loss: 0.53230   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 93   Avg_Loss: 0.39113   Acc: 519/625 (83.040%)\n",
            "Eval:  Avg_Loss: 0.53313   Acc: 175/209 (83.732%)\n",
            "Train Epoch: 94   Avg_Loss: 0.38398   Acc: 519/625 (83.040%)\n",
            "Eval:  Avg_Loss: 0.54288   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 95   Avg_Loss: 0.38096   Acc: 524/625 (83.840%)\n",
            "Eval:  Avg_Loss: 0.56877   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 96   Avg_Loss: 0.38856   Acc: 521/625 (83.360%)\n",
            "Eval:  Avg_Loss: 0.53687   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 97   Avg_Loss: 0.38609   Acc: 515/625 (82.400%)\n",
            "Eval:  Avg_Loss: 0.54649   Acc: 173/209 (82.775%)\n",
            "Train Epoch: 98   Avg_Loss: 0.38225   Acc: 517/625 (82.720%)\n",
            "Eval:  Avg_Loss: 0.54315   Acc: 173/209 (82.775%)\n",
            "Train Epoch: 99   Avg_Loss: 0.38282   Acc: 522/625 (83.520%)\n",
            "Eval:  Avg_Loss: 0.55040   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 100   Avg_Loss: 0.38031   Acc: 524/625 (83.840%)\n",
            "Eval:  Avg_Loss: 0.56127   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 101   Avg_Loss: 0.38824   Acc: 518/625 (82.880%)\n",
            "Eval:  Avg_Loss: 0.53876   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 102   Avg_Loss: 0.38283   Acc: 518/625 (82.880%)\n",
            "Eval:  Avg_Loss: 0.55970   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 103   Avg_Loss: 0.38160   Acc: 518/625 (82.880%)\n",
            "Eval:  Avg_Loss: 0.54847   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 104   Avg_Loss: 0.38226   Acc: 520/625 (83.200%)\n",
            "Eval:  Avg_Loss: 0.52014   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 105   Avg_Loss: 0.38885   Acc: 519/625 (83.040%)\n",
            "Eval:  Avg_Loss: 0.51331   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 106   Avg_Loss: 0.37916   Acc: 524/625 (83.840%)\n",
            "Eval:  Avg_Loss: 0.53838   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 107   Avg_Loss: 0.37845   Acc: 521/625 (83.360%)\n",
            "Eval:  Avg_Loss: 0.55165   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 108   Avg_Loss: 0.37550   Acc: 522/625 (83.520%)\n",
            "Eval:  Avg_Loss: 0.54779   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 109   Avg_Loss: 0.37867   Acc: 525/625 (84.000%)\n",
            "Eval:  Avg_Loss: 0.53600   Acc: 172/209 (82.297%)\n",
            "Train Epoch: 110   Avg_Loss: 0.37979   Acc: 523/625 (83.680%)\n",
            "Eval:  Avg_Loss: 0.55090   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 111   Avg_Loss: 0.37485   Acc: 524/625 (83.840%)\n",
            "Eval:  Avg_Loss: 0.54121   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 112   Avg_Loss: 0.37982   Acc: 523/625 (83.680%)\n",
            "Eval:  Avg_Loss: 0.55718   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 113   Avg_Loss: 0.37878   Acc: 522/625 (83.520%)\n",
            "Eval:  Avg_Loss: 0.54900   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 114   Avg_Loss: 0.37923   Acc: 525/625 (84.000%)\n",
            "Eval:  Avg_Loss: 0.54695   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 115   Avg_Loss: 0.37780   Acc: 523/625 (83.680%)\n",
            "Eval:  Avg_Loss: 0.54718   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 116   Avg_Loss: 0.37115   Acc: 526/625 (84.160%)\n",
            "Eval:  Avg_Loss: 0.56426   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 117   Avg_Loss: 0.37362   Acc: 523/625 (83.680%)\n",
            "Eval:  Avg_Loss: 0.56742   Acc: 173/209 (82.775%)\n",
            "Train Epoch: 118   Avg_Loss: 0.37318   Acc: 528/625 (84.480%)\n",
            "Eval:  Avg_Loss: 0.55128   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 119   Avg_Loss: 0.37140   Acc: 529/625 (84.640%)\n",
            "Eval:  Avg_Loss: 0.57765   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 120   Avg_Loss: 0.39555   Acc: 518/625 (82.880%)\n",
            "Eval:  Avg_Loss: 0.59119   Acc: 162/209 (77.512%)\n",
            "Train Epoch: 121   Avg_Loss: 0.40812   Acc: 515/625 (82.400%)\n",
            "Eval:  Avg_Loss: 0.55666   Acc: 172/209 (82.297%)\n",
            "Train Epoch: 122   Avg_Loss: 0.37654   Acc: 521/625 (83.360%)\n",
            "Eval:  Avg_Loss: 0.56239   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 123   Avg_Loss: 0.37720   Acc: 524/625 (83.840%)\n",
            "Eval:  Avg_Loss: 0.57083   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 124   Avg_Loss: 0.37079   Acc: 529/625 (84.640%)\n",
            "Eval:  Avg_Loss: 0.56524   Acc: 172/209 (82.297%)\n",
            "Train Epoch: 125   Avg_Loss: 0.36769   Acc: 526/625 (84.160%)\n",
            "Eval:  Avg_Loss: 0.55098   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 126   Avg_Loss: 0.37344   Acc: 521/625 (83.360%)\n",
            "Eval:  Avg_Loss: 0.57150   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 127   Avg_Loss: 0.37036   Acc: 524/625 (83.840%)\n",
            "Eval:  Avg_Loss: 0.55862   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 128   Avg_Loss: 0.36940   Acc: 531/625 (84.960%)\n",
            "Eval:  Avg_Loss: 0.56339   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 129   Avg_Loss: 0.36889   Acc: 525/625 (84.000%)\n",
            "Eval:  Avg_Loss: 0.57110   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 130   Avg_Loss: 0.36838   Acc: 527/625 (84.320%)\n",
            "Eval:  Avg_Loss: 0.57246   Acc: 173/209 (82.775%)\n",
            "Train Epoch: 131   Avg_Loss: 0.37728   Acc: 519/625 (83.040%)\n",
            "Eval:  Avg_Loss: 0.57229   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 132   Avg_Loss: 0.36873   Acc: 526/625 (84.160%)\n",
            "Eval:  Avg_Loss: 0.55672   Acc: 172/209 (82.297%)\n",
            "Train Epoch: 133   Avg_Loss: 0.38250   Acc: 523/625 (83.680%)\n",
            "Eval:  Avg_Loss: 0.56772   Acc: 173/209 (82.775%)\n",
            "Train Epoch: 134   Avg_Loss: 0.36701   Acc: 519/625 (83.040%)\n",
            "Eval:  Avg_Loss: 0.56395   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 135   Avg_Loss: 0.36843   Acc: 526/625 (84.160%)\n",
            "Eval:  Avg_Loss: 0.56318   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 136   Avg_Loss: 0.36745   Acc: 530/625 (84.800%)\n",
            "Eval:  Avg_Loss: 0.55318   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 137   Avg_Loss: 0.37170   Acc: 526/625 (84.160%)\n",
            "Eval:  Avg_Loss: 0.57342   Acc: 173/209 (82.775%)\n",
            "Train Epoch: 138   Avg_Loss: 0.36693   Acc: 523/625 (83.680%)\n",
            "Eval:  Avg_Loss: 0.54451   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 139   Avg_Loss: 0.37549   Acc: 522/625 (83.520%)\n",
            "Eval:  Avg_Loss: 0.57744   Acc: 173/209 (82.775%)\n",
            "Train Epoch: 140   Avg_Loss: 0.38330   Acc: 522/625 (83.520%)\n",
            "Eval:  Avg_Loss: 0.56649   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 141   Avg_Loss: 0.37651   Acc: 524/625 (83.840%)\n",
            "Eval:  Avg_Loss: 0.57640   Acc: 173/209 (82.775%)\n",
            "Train Epoch: 142   Avg_Loss: 0.37039   Acc: 527/625 (84.320%)\n",
            "Eval:  Avg_Loss: 0.57092   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 143   Avg_Loss: 0.36077   Acc: 523/625 (83.680%)\n",
            "Eval:  Avg_Loss: 0.56293   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 144   Avg_Loss: 0.36739   Acc: 526/625 (84.160%)\n",
            "Eval:  Avg_Loss: 0.56856   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 145   Avg_Loss: 0.36579   Acc: 523/625 (83.680%)\n",
            "Eval:  Avg_Loss: 0.57890   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 146   Avg_Loss: 0.36900   Acc: 516/625 (82.560%)\n",
            "Eval:  Avg_Loss: 0.57123   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 147   Avg_Loss: 0.36065   Acc: 532/625 (85.120%)\n",
            "Eval:  Avg_Loss: 0.57330   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 148   Avg_Loss: 0.36390   Acc: 525/625 (84.000%)\n",
            "Eval:  Avg_Loss: 0.57516   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 149   Avg_Loss: 0.36486   Acc: 523/625 (83.680%)\n",
            "Eval:  Avg_Loss: 0.58359   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 150   Avg_Loss: 0.36613   Acc: 526/625 (84.160%)\n",
            "Eval:  Avg_Loss: 0.57229   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 151   Avg_Loss: 0.36857   Acc: 529/625 (84.640%)\n",
            "Eval:  Avg_Loss: 0.56589   Acc: 173/209 (82.775%)\n",
            "Train Epoch: 152   Avg_Loss: 0.36144   Acc: 528/625 (84.480%)\n",
            "Eval:  Avg_Loss: 0.58674   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 153   Avg_Loss: 0.36259   Acc: 527/625 (84.320%)\n",
            "Eval:  Avg_Loss: 0.58041   Acc: 174/209 (83.254%)\n",
            "Train Epoch: 154   Avg_Loss: 0.35757   Acc: 529/625 (84.640%)\n",
            "Eval:  Avg_Loss: 0.59868   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 155   Avg_Loss: 0.37789   Acc: 529/625 (84.640%)\n",
            "Eval:  Avg_Loss: 0.63068   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 156   Avg_Loss: 0.38563   Acc: 515/625 (82.400%)\n",
            "Eval:  Avg_Loss: 0.57862   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 157   Avg_Loss: 0.36109   Acc: 529/625 (84.640%)\n",
            "Eval:  Avg_Loss: 0.59061   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 158   Avg_Loss: 0.36204   Acc: 524/625 (83.840%)\n",
            "Eval:  Avg_Loss: 0.54645   Acc: 172/209 (82.297%)\n",
            "Train Epoch: 159   Avg_Loss: 0.36471   Acc: 523/625 (83.680%)\n",
            "Eval:  Avg_Loss: 0.57041   Acc: 172/209 (82.297%)\n",
            "Train Epoch: 160   Avg_Loss: 0.37116   Acc: 517/625 (82.720%)\n",
            "Eval:  Avg_Loss: 0.57626   Acc: 166/209 (79.426%)\n",
            "Train Epoch: 161   Avg_Loss: 0.36534   Acc: 528/625 (84.480%)\n",
            "Eval:  Avg_Loss: 0.58922   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 162   Avg_Loss: 0.36201   Acc: 527/625 (84.320%)\n",
            "Eval:  Avg_Loss: 0.55407   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 163   Avg_Loss: 0.36027   Acc: 532/625 (85.120%)\n",
            "Eval:  Avg_Loss: 0.57359   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 164   Avg_Loss: 0.36014   Acc: 532/625 (85.120%)\n",
            "Eval:  Avg_Loss: 0.60482   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 165   Avg_Loss: 0.38217   Acc: 520/625 (83.200%)\n",
            "Eval:  Avg_Loss: 0.53616   Acc: 173/209 (82.775%)\n",
            "Train Epoch: 166   Avg_Loss: 0.36397   Acc: 523/625 (83.680%)\n",
            "Eval:  Avg_Loss: 0.53051   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 167   Avg_Loss: 0.37428   Acc: 518/625 (82.880%)\n",
            "Eval:  Avg_Loss: 0.53892   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 168   Avg_Loss: 0.36007   Acc: 523/625 (83.680%)\n",
            "Eval:  Avg_Loss: 0.55759   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 169   Avg_Loss: 0.38501   Acc: 510/625 (81.600%)\n",
            "Eval:  Avg_Loss: 0.55209   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 170   Avg_Loss: 0.37030   Acc: 527/625 (84.320%)\n",
            "Eval:  Avg_Loss: 0.56173   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 171   Avg_Loss: 0.35711   Acc: 527/625 (84.320%)\n",
            "Eval:  Avg_Loss: 0.55932   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 172   Avg_Loss: 0.36025   Acc: 524/625 (83.840%)\n",
            "Eval:  Avg_Loss: 0.57309   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 173   Avg_Loss: 0.35455   Acc: 527/625 (84.320%)\n",
            "Eval:  Avg_Loss: 0.55195   Acc: 172/209 (82.297%)\n",
            "Train Epoch: 174   Avg_Loss: 0.35400   Acc: 525/625 (84.000%)\n",
            "Eval:  Avg_Loss: 0.57253   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 175   Avg_Loss: 0.35990   Acc: 531/625 (84.960%)\n",
            "Eval:  Avg_Loss: 0.57849   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 176   Avg_Loss: 0.35352   Acc: 531/625 (84.960%)\n",
            "Eval:  Avg_Loss: 0.58131   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 177   Avg_Loss: 0.35141   Acc: 532/625 (85.120%)\n",
            "Eval:  Avg_Loss: 0.56887   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 178   Avg_Loss: 0.35254   Acc: 527/625 (84.320%)\n",
            "Eval:  Avg_Loss: 0.58724   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 179   Avg_Loss: 0.35837   Acc: 527/625 (84.320%)\n",
            "Eval:  Avg_Loss: 0.58715   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 180   Avg_Loss: 0.34941   Acc: 530/625 (84.800%)\n",
            "Eval:  Avg_Loss: 0.56889   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 181   Avg_Loss: 0.35491   Acc: 521/625 (83.360%)\n",
            "Eval:  Avg_Loss: 0.57264   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 182   Avg_Loss: 0.35020   Acc: 528/625 (84.480%)\n",
            "Eval:  Avg_Loss: 0.56234   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 183   Avg_Loss: 0.35069   Acc: 531/625 (84.960%)\n",
            "Eval:  Avg_Loss: 0.57283   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 184   Avg_Loss: 0.35104   Acc: 535/625 (85.600%)\n",
            "Eval:  Avg_Loss: 0.56313   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 185   Avg_Loss: 0.35064   Acc: 529/625 (84.640%)\n",
            "Eval:  Avg_Loss: 0.58914   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 186   Avg_Loss: 0.34697   Acc: 526/625 (84.160%)\n",
            "Eval:  Avg_Loss: 0.60237   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 187   Avg_Loss: 0.36213   Acc: 529/625 (84.640%)\n",
            "Eval:  Avg_Loss: 0.59650   Acc: 172/209 (82.297%)\n",
            "Train Epoch: 188   Avg_Loss: 0.35634   Acc: 530/625 (84.800%)\n",
            "Eval:  Avg_Loss: 0.57217   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 189   Avg_Loss: 0.35407   Acc: 530/625 (84.800%)\n",
            "Eval:  Avg_Loss: 0.60955   Acc: 169/209 (80.861%)\n",
            "Train Epoch: 190   Avg_Loss: 0.34925   Acc: 534/625 (85.440%)\n",
            "Eval:  Avg_Loss: 0.58119   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 191   Avg_Loss: 0.35853   Acc: 528/625 (84.480%)\n",
            "Eval:  Avg_Loss: 0.59773   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 192   Avg_Loss: 0.34859   Acc: 529/625 (84.640%)\n",
            "Eval:  Avg_Loss: 0.59376   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 193   Avg_Loss: 0.34674   Acc: 533/625 (85.280%)\n",
            "Eval:  Avg_Loss: 0.59241   Acc: 173/209 (82.775%)\n",
            "Train Epoch: 194   Avg_Loss: 0.35015   Acc: 535/625 (85.600%)\n",
            "Eval:  Avg_Loss: 0.60477   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 195   Avg_Loss: 0.39336   Acc: 521/625 (83.360%)\n",
            "Eval:  Avg_Loss: 0.67202   Acc: 167/209 (79.904%)\n",
            "Train Epoch: 196   Avg_Loss: 0.36253   Acc: 528/625 (84.480%)\n",
            "Eval:  Avg_Loss: 0.59296   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 197   Avg_Loss: 0.34861   Acc: 526/625 (84.160%)\n",
            "Eval:  Avg_Loss: 0.59867   Acc: 170/209 (81.340%)\n",
            "Train Epoch: 198   Avg_Loss: 0.35044   Acc: 528/625 (84.480%)\n",
            "Eval:  Avg_Loss: 0.59611   Acc: 168/209 (80.383%)\n",
            "Train Epoch: 199   Avg_Loss: 0.35056   Acc: 534/625 (85.440%)\n",
            "Eval:  Avg_Loss: 0.60069   Acc: 171/209 (81.818%)\n",
            "Train Epoch: 200   Avg_Loss: 0.35008   Acc: 531/625 (84.960%)\n",
            "Eval:  Avg_Loss: 0.62132   Acc: 171/209 (81.818%)\n",
            "\n",
            "\n",
            "\n",
            "Optimization ended.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "86OZRLrjavmd"
      },
      "source": [
        "## Interpreting the output of the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mklvQruYavme",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "f024b7c1-a76d-4e3c-d3bb-c961cb0b0f03"
      },
      "source": [
        "# Activate the evaluation mode\n",
        "neural_net = neural_net.eval()\n",
        "\n",
        "# Select the first 10 data points of the validation set\n",
        "data, target = val_dataset[0:10]\n",
        "data = data.to(device)\n",
        "\n",
        "# Executing the neural network\n",
        "output = neural_net(data)   # equivalent to neural_net.forward(data)\n",
        "\n",
        "# Transform the output into a probability distribution with a softmax function\n",
        "output_proba = F.softmax(output, dim=1) # To complete.\n",
        "\n",
        "# Print the probability\n",
        "print(output_proba)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.9359, 0.0641],\n",
            "        [0.6735, 0.3265],\n",
            "        [0.5870, 0.4130],\n",
            "        [0.5791, 0.4209],\n",
            "        [0.8722, 0.1278],\n",
            "        [0.9845, 0.0155],\n",
            "        [0.8433, 0.1567],\n",
            "        [0.8274, 0.1726],\n",
            "        [0.0106, 0.9894],\n",
            "        [0.0208, 0.9792]], device='cuda:0', grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RvIEqKt0qjeT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "5bccb1bc-245e-4d53-9b2d-1550a47a649d"
      },
      "source": [
        "# For each example, retrieve the class with the highest probability.\n",
        "_, prediction = torch.max(output_proba, dim=1)\n",
        "\n",
        "print(\"Model predictions\")\n",
        "print(prediction)\n",
        "\n",
        "print(\"Targets\")\n",
        "print(target)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model predictions\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 1, 1], device='cuda:0')\n",
            "Targets\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V11J3Jihavmy"
      },
      "source": [
        "## Visualizing of the learning curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j9_9C_tXavmz"
      },
      "source": [
        "The visualization of the learning curve allows to detect possible problems that may have occurred during learning, for example, overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iNcbpl0tavm0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "dc4914b1-264e-42d6-d118-d3eb50d1bbb4"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "x = list(range(len(train_losses)))\n",
        "\n",
        "ax = plt.subplot(111)\n",
        "plt.plot(x, train_losses, 'r', label=\"Train\")\n",
        "plt.plot(x, val_losses, 'g', label=\"Validation\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Cross-entropy loss')\n",
        "plt.grid()\n",
        "leg = plt.legend(loc='best', ncol=2, mode=\"expand\", shadow=False, fancybox=False)\n",
        "leg.get_frame().set_alpha(0.99)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g-VGQ2pMavm4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "9757bc92-4ea1-4b65-e458-78eb44731c3a"
      },
      "source": [
        "x = list(range(len(train_accuracies)))\n",
        "\n",
        "ax = plt.subplot(111)\n",
        "plt.plot(x, train_accuracies, 'r', label=\"Train\")\n",
        "plt.plot(x, val_accuracies, 'g', label=\"Validation\")\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid()\n",
        "leg = plt.legend(loc='best', ncol=2, mode=\"expand\", shadow=False, fancybox=False)\n",
        "leg.get_frame().set_alpha(0.99)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8oktkpkuqjet"
      },
      "source": [
        "**Questions**:\n",
        "\n",
        "*   What do you observe in the previous graphs?\n",
        "*   At which epoch is it interesting to retrieve the model parameters for inference?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwhRt39yzug-",
        "colab_type": "text"
      },
      "source": [
        "... # To complete.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XK_eUsq3avm8"
      },
      "source": [
        "# How to evaluate a model on the test set?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4UREO5elavm8"
      },
      "source": [
        "We can finally evaluate our model on our test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pPWvDM-qavm8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1cf2d52f-bb0d-4419-bd1e-01e2e3bbfeb2"
      },
      "source": [
        "test_loss, test_acc = evaluate(neural_net, test_loader, device)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Eval:  Avg_Loss: 0.54583   Acc: 163/209 (77.990%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EvP_-KUwqjez"
      },
      "source": [
        "**Questions**:\n",
        "\n",
        "a) Compare validation and test metrics. <br/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov4CKUFh5tun",
        "colab_type": "text"
      },
      "source": [
        "... # To complete.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfykotIrJV0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}